{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomaslam/FinetuningLLMs_Project_1/blob/main/FinetuningLLMs_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1 Learning Goals\n",
        "\n",
        "1. **Understand Encoder and Decoder Models**: Get familiar with both types of models and how they work in text analytics.\n",
        "2. **Model Selection**: Learn how to choose the right model for a specific task.\n",
        "3. **Apply Few-Shot Learning**: Use few-shot learning techniques to enhance model performance.\n",
        "4. **Evaluate Models**: Use metrics like accuracy and F1-score to assess model effectiveness.\n",
        "5. **Compare Models**: Learn how to draw comparisons between different models based on evaluation metrics."
      ],
      "metadata": {
        "id": "8jVCEL9l4VQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to this project notebook! In this exercise, we aim to explore and understand the capabilities of different language models, specifically focusing on encoder and decoder architectures.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will be working with the Financial Phrasebank dataset, which consists of sentences from English language financial news categorized by sentiment. This dataset provides a real-world scenario for us to test and evaluate the selected models.\n",
        "\n",
        "Let's get started!\n"
      ],
      "metadata": {
        "id": "wkQG3PHj4t93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Before we dive into the project, we need to set up our environment. This involves installing the necessary Python libraries and importing them for use in this notebook.\n",
        "\n",
        "## Installing Libraries\n",
        "\n",
        "Run the following cell to install the required libraries.\n"
      ],
      "metadata": {
        "id": "kXaHpbiL7yro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install einops --quiet"
      ],
      "metadata": {
        "id": "M1WW3nMn4tVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n",
        "\n",
        "After installation, import the libraries needed for this project. This typically includes Transformers for handling our encoder and decoder models, and scikit-learn for evaluation metrics."
      ],
      "metadata": {
        "id": "IKrSjFwn8BIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names, load_dataset\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from typing import List, Dict, Any, Union, Generator, Callable\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
        "import openai\n",
        "from getpass import getpass\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import unittest\n",
        "from unittest.mock import Mock\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_default_device(torch_device)"
      ],
      "metadata": {
        "id": "GpyX9Bdv8Ahi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "We'll be working with the Financial Phrasebank dataset for this project. This dataset contains sentences extracted from financial news articles, each labeled with a sentiment category. The dataset serves as a real-world testbed for evaluating the performance of various language models in the domain of financial text analytics.\n",
        "\n",
        "## Loading the Dataset\n",
        "\n",
        "We'll be using the Financial Phrasebank dataset available on Hugging Face. This dataset comes with a few different configurations based on the agreement rate among annotators. The configurations available are:\n",
        "\n",
        "- `sentences_allagree`\n",
        "- `sentences_75agree`\n",
        "- `sentences_66agree`\n",
        "- `sentences_50agree`\n",
        "\n",
        "For this project, we'll use the `sentences_50agree` configuration.\n",
        "\n",
        "## Splitting Data\n",
        "\n",
        "The dataset from Hugging Face doesn't come pre-split into training and test sets; it only has a 'train' split. Therefore, we'll need to create our own test set.\n",
        "\n",
        "Run the following code to load the dataset and then split it into training and test sets."
      ],
      "metadata": {
        "id": "31p0-kTR8bsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available configurations\n",
        "configs = get_dataset_config_names(\"financial_phrasebank\")\n",
        "\n",
        "# Load the dataset with the 'sentences_50agree' configuration\n",
        "phrasebank = load_dataset(\"financial_phrasebank\", \"sentences_50agree\")\n",
        "\n",
        "# Split the 'train' data into training and test sets\n",
        "phrasebank_split = phrasebank[\"train\"].train_test_split(test_size=0.1, shuffle=True)"
      ],
      "metadata": {
        "id": "W925WpK08y0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Statistics\n",
        "\n",
        "Once the dataset is loaded, it's useful to get a quick sense of its size and structure. Specifically, we'll look at the number of samples and the distribution of sentiment labels.  Note that the dataset labels are 0, 1, and 2 which correspond to negative, neutral, and positive (respectively).\n",
        "\n",
        "**Note to Students**: In this project, we are working with pre-trained models, which means we won't be using the training set to update the model weights through training. Instead, the training set will serve a different purpose: we will use it for one-shot or few-shot learning. This approach allows us to leverage a small subset of labeled examples to guide the model's predictions. We will then evaluate how these adjustments impact the model's performance on the test set."
      ],
      "metadata": {
        "id": "YmJd_uDy-kLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of samples in the training set\n",
        "num_samples_train = len(phrasebank_split['train'])\n",
        "# Count the number of samples in the test set\n",
        "num_samples_test = len(phrasebank_split['test'])\n",
        "\n",
        "# Display basic statistics\n",
        "print(f\"Number of samples in training set: {num_samples_train}\")\n",
        "print(f\"Number of samples in test set: {num_samples_test}\")\n",
        "\n",
        "# Extract labels from the training set and count occurrences\n",
        "label_list = [example['label'] for example in phrasebank_split['train']]\n",
        "label_counts = Counter(label_list)\n",
        "# Create the histogram\n",
        "labels, counts = zip(*label_counts.items())\n",
        "\n",
        "# Note: In the dataset, 0 represents 'negative', 1 represents 'neutral', and 2 represents 'positive'\n",
        "label_names = ['Negative (0)', 'Neutral (1)', 'Positive (2)']\n",
        "plt.bar(label_names, counts)\n",
        "plt.xlabel('Sentiment Labels')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Label Distribution in Training Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SB7zqv3t8y2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data\n",
        "\n",
        "After loading and splitting the dataset, it's a good idea to take a quick look at some sample entries. This will give you a sense of what the data actually looks like.\n",
        "\n",
        "Run the following code snippet to display a few random samples from the training set:"
      ],
      "metadata": {
        "id": "1AylXBtYG_fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly pick 5 samples from the training set\n",
        "random_samples = random.choices(phrasebank_split['train'], k=5)\n",
        "\n",
        "# Display the samples\n",
        "for i, sample in enumerate(random_samples):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Text: {sample['sentence']}\")\n",
        "    print(f\"Label: {sample['label']} (0: Negative, 1: Neutral, 2: Positive)\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "j-0A5fQ48y4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-Shot Learning\n",
        "\n",
        "Few-shot learning is a pivotal aspect of this project, specifically for flexing the capabilities of decoder models like Phi-1.5 and ChatGPT. In contrast to encoder models, which are highly specialized but not very adaptable, few-shot learning allows decoder models to perform different tasks without requiring any retraining.\n",
        "\n",
        "In this project, you'll get hands-on experience by writing your own function to prepare few-shot examples. These examples will come from the training set and will \"prime\" the models to perform sentiment analysis on the test set. Essentially, you'll use a handful of examples to guide the model's understanding of what 'Negative,' 'Neutral,' and 'Positive' sentiments look like.\n",
        "\n",
        "We will be using a python [generator](https://docs.python.org/3/reference/simple_stmts.html#yield) to create the examples on the fly.\n",
        "\n",
        "Each example will have the following format:\n",
        "\n",
        "```text\n",
        "Classify the sentiment (negative, neutral, positive) of the following statement:\n",
        "[This is a negative sample from the phrasebank.]\n",
        "Reponse: negative\n",
        "```\n",
        "For the test phrase \"Export declined by 6 percent to 16.4 million liters\", a zero shot example would look like:\n",
        "```text\n",
        "Classify the sentiment (negative, neutral, positive) of the following statement:\n",
        "[Export declined by 6 percent to 16.4 million liters]\n",
        "Reponse:\n",
        "```\n",
        "A one-shot example would look like:\n",
        "```text\n",
        "Classify the sentiment (negative, neutral, positive) of the following statement:\n",
        "[Finnish Raisio ( Diagnostics ) is launching new DNA-based quick tests to ensure the safety of food .]\n",
        "Reponse: positive\n",
        "[Export declined by 6 percent to 16.4 million liters]\n",
        "Reponse:\n",
        "```\n",
        "A two-shot example would look like:\n",
        "```text\n",
        "Classify the sentiment (negative, neutral, positive) of the following statement:\n",
        "[Finnish meat company Atria can no longer promise a sufficient amount of domestic beef to its customers .]\n",
        "Reponse: negative\n",
        "[Finnish Raisio ( Diagnostics ) is launching new DNA-based quick tests to ensure the safety of food .]\n",
        "Reponse: positive\n",
        "[Export declined by 6 percent to 16.4 million liters]\n",
        "Reponse:\n",
        "```\n",
        "\n",
        "Let's give this a shot!"
      ],
      "metadata": {
        "id": "-b1Y5qQjLXtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_n_shot_examples(prompt: str,\n",
        "                             train_set: List[Dict[str, Any]],\n",
        "                             test_set: List[Dict[str, Any]],\n",
        "                             n: int) -> Generator[str, None, None]:\n",
        "    \"\"\"\n",
        "    Generates simple n-shot examples using sequential samples from the training set.\n",
        "\n",
        "    This function selects the next 'n' examples from the training set and appends them\n",
        "    to each query from the test set to create n-shot learning examples.\n",
        "\n",
        "    Args:\n",
        "        prompt: The prompt text to be used for the few-shot examples.\n",
        "        train_set: The training set, a list of dictionaries containing 'sentence' and 'label' keys.\n",
        "        test_set: The test set, a list of dictionaries containing 'sentence' and 'label' keys.\n",
        "        n: The number of examples to select from the training set.\n",
        "\n",
        "    Yields:\n",
        "        A sample, label representing an n-shot example.\n",
        "    \"\"\"\n",
        "\n",
        "    sentiment_labels = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # REPLACE None WITH YOUR CODE\n",
        "\n",
        "    # Calculate how many samples can be created from the test set length, train set length, and n\n",
        "    if n != 0 and len(train_set) >= n:\n",
        "        n_samples = min(None, None)\n",
        "    else:\n",
        "        n_samples = None\n",
        "\n",
        "    # Create a generator that iterates over the examples\n",
        "    test_gen = iter(None)\n",
        "    train_gen = iter(None)\n",
        "\n",
        "    # Generate n-shot examples\n",
        "    for _ in range(n_samples):\n",
        "        test_sample = next(test_gen)\n",
        "        n_shot_example = f\"{prompt}:\\n\"\n",
        "\n",
        "        # Add the n-shot examples to the prompt\n",
        "        for _ in range(n):\n",
        "            train_sample = next(train_gen)\n",
        "            n_shot_example += f\"[{train_sample['sentence']}]\\nResponse: {sentiment_labels[train_sample['label']]}\\n\"\n",
        "\n",
        "        # Add the final test sentence without its label\n",
        "        n_shot_example += f\"[{None}]\\nResponse:\"\n",
        "\n",
        "        # Use the sentiment_labels dict to convert the numeric (0,1,2) to a text label\n",
        "        sentiment_label = None\n",
        "\n",
        "        yield n_shot_example, sentiment_label\n",
        "\n",
        "    #########################\n",
        "    # FINISH YOUR CODE HERE #\n",
        "    #########################\n",
        "\n",
        "\n",
        "# Generate 2-shot examples\n",
        "n_shot_generator = generate_n_shot_examples(\"Classify the sentiment (negative, neutral, positive) of the following statement\",\n",
        "                                            phrasebank_split['train'], phrasebank_split['test'], 2)\n",
        "\n",
        "# Display a couple generated n-shot examples\n",
        "for _ in range(2):\n",
        "    sample = next(n_shot_generator)\n",
        "    print(sample[0])\n",
        "    print()\n",
        "    print(sample[1])\n",
        "    print()"
      ],
      "metadata": {
        "id": "WPVL0GCkLpYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Your Code!\n",
        "class TestGenerateNShotExamplesUpdated(unittest.TestCase):\n",
        "\n",
        "    def test_generate_n_shot_examples(self):\n",
        "        \"\"\"\n",
        "        Test the functionality of generate_n_shot_examples using static data.\n",
        "        \"\"\"\n",
        "        prompt = \"Classify the sentiment (negative, neutral, positive) of the following statement\"\n",
        "        train_set: List[Dict[str, Any]] = [\n",
        "            {'sentence': 'I love this product', 'label': 2},\n",
        "            {'sentence': 'This is just okay', 'label': 1},\n",
        "            {'sentence': 'I hate this', 'label': 0},\n",
        "            {'sentence': 'This is amazing', 'label': 2}\n",
        "        ]\n",
        "        test_set: List[Dict[str, Any]] = [\n",
        "            {'sentence': 'This is terrible', 'label': 0},\n",
        "            {'sentence': 'This is fantastic', 'label': 2}\n",
        "        ]\n",
        "        n = 2\n",
        "\n",
        "        expected_output = [\n",
        "            f\"{prompt}:\\n[I love this product]\\nResponse: positive\\n[This is just okay]\\nResponse: neutral\\n[This is terrible]\\nResponse:\",\n",
        "            f\"{prompt}:\\n[I hate this]\\nResponse: negative\\n[This is amazing]\\nResponse: positive\\n[This is fantastic]\\nResponse:\"\n",
        "        ]\n",
        "        expected_labels = ['negative', 'positive']\n",
        "\n",
        "        gen: Generator[str, None, None] = generate_n_shot_examples(prompt, train_set, test_set, n)\n",
        "\n",
        "        for prompt, label in zip(expected_output, expected_labels):\n",
        "            produced = next(gen)\n",
        "            self.assertEqual(produced[0], prompt)\n",
        "            self.assertEqual(produced[1], label)\n",
        "\n",
        "        # Test for exhaustion of generator\n",
        "        with self.assertRaises(StopIteration):\n",
        "            next(gen)\n",
        "\n",
        "    def test_small_train_set(self):\n",
        "        \"\"\"\n",
        "        Test the case where the train set is smaller than the test set.\n",
        "        \"\"\"\n",
        "        prompt = \"Classify the sentiment\"\n",
        "        train_set: List[Dict[str, Any]] = [\n",
        "            {'sentence': 'I love this', 'label': 2}\n",
        "        ]\n",
        "        test_set: List[Dict[str, Any]] = [\n",
        "            {'sentence': 'This is bad', 'label': 0},\n",
        "            {'sentence': 'This is good', 'label': 2}\n",
        "        ]\n",
        "        n = 1\n",
        "\n",
        "        # Since there's only one training example, it should be repeated for each test example.\n",
        "        expected_output = [\n",
        "            f\"{prompt}:\\n[I love this]\\nResponse: positive\\n[This is bad]\\nResponse:\",\n",
        "        ]\n",
        "\n",
        "        gen: Generator[str, None, None] = generate_n_shot_examples(prompt, train_set, test_set, n)\n",
        "\n",
        "        for expected in expected_output:\n",
        "            self.assertEqual(next(gen)[0], expected)\n",
        "\n",
        "        # Test for exhaustion of generator\n",
        "        with self.assertRaises(StopIteration):\n",
        "            next(gen)\n",
        "\n",
        "    def test_small_test_set(self):\n",
        "        \"\"\"\n",
        "        Test the case where the test set is smaller than the train set.\n",
        "        \"\"\"\n",
        "        prompt = \"Classify the sentiment\"\n",
        "        train_set: List[Dict[str, Any]] = [\n",
        "            {'sentence': 'I love this', 'label': 2},\n",
        "            {'sentence': 'This is okay', 'label': 1},\n",
        "            {'sentence': 'I hate this', 'label': 0}\n",
        "        ]\n",
        "        test_set: List[Dict[str, Any]] = [\n",
        "            {'sentence': 'This is bad', 'label': 0}\n",
        "        ]\n",
        "        n = 2\n",
        "\n",
        "        expected_output = [\n",
        "            f\"{prompt}:\\n[I love this]\\nResponse: positive\\n[This is okay]\\nResponse: neutral\\n[This is bad]\\nResponse:\"\n",
        "        ]\n",
        "\n",
        "        gen: Generator[str, None, None] = generate_n_shot_examples(prompt, train_set, test_set, n)\n",
        "\n",
        "        for expected in expected_output:\n",
        "            self.assertEqual(next(gen)[0], expected)\n",
        "\n",
        "        # Test for exhaustion of generator\n",
        "        with self.assertRaises(StopIteration):\n",
        "            next(gen)\n",
        "\n",
        "# Run the tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestGenerateNShotExamplesUpdated))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2q_97QEIxVQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Overview & Initialization\n",
        "\n",
        "## Why Encoder and Decoder Models?\n",
        "\n",
        "In this project, we focus on two distinct types of language models: encoder models and decoder models. You may wonder why we're using both. Encoder models like Roberta and FinBERT are highly specialized for tasks like sentiment analysis. They take text input and transform it into a fixed-size vector that captures essential information. While effective for their specialized tasks, they aren't very flexible for others.\n",
        "\n",
        "Decoder models such as ChatGPT and Phi-1.5 are fundamentally designed to generate text. These models can be adapted for various tasks, like sentiment analysis, through a technique known as few-shot learning. This makes them versatile, even without task-specific training.\n",
        "\n",
        "By exploring both encoder and decoder models, we aim to provide a well-rounded understanding of their capabilities and constraints. This sets the stage for a comprehensive evaluation later in the notebook, especially when we change tasks to demonstrate the adaptability of decoder models.\n",
        "\n",
        "## Running models form different sources\n",
        "\n",
        "We will execute both open and closed source models:  Roberta, FinBERT, and Phi-1.5 are open source, while GPT-3.5 is closed source and only accessible via API.\n",
        "\n",
        "## Coding Examples\n",
        "\n",
        "First, we'll show you how to download and run inference on the encoder models. These will include the general-purpose text classification model cardiffnlp/twitter-roberta-base-sentiment-latest and the financial news-focused model ProsusAI/finbert.\n"
      ],
      "metadata": {
        "id": "fiE35YdpHkaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_names = phrasebank[\"train\"].features[\"label\"].names\n",
        "\n",
        "# Initialize Roberta tokenizer and model\n",
        "checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "roberta_config = AutoConfig.from_pretrained(checkpoint)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Initialize the Finbert tokenizer and model\n",
        "checkpoint = \"ProsusAI/finbert\"\n",
        "finbert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "finbert_config = AutoConfig.from_pretrained(checkpoint)\n",
        "finbert_model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "X8zxs8T9k1dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = ['negative', 'neutral', 'positive']\n",
        "\n",
        "# Run inference on a positive example\n",
        "text = \"Revenue grew 12 percent to (  x20ac ) 3.6 billion ( US$ 4.5 billion ) .\"\n",
        "finbert_inputs = finbert_tokenizer(text, return_tensors=\"pt\")\n",
        "finbert_outputs = finbert_model(**finbert_inputs)\n",
        "finbert_prediction = finbert_outputs.logits.argmax(dim=1).item()\n",
        "finbert_sentiment = finbert_config.id2label[finbert_prediction]\n",
        "\n",
        "roberta_inputs = roberta_tokenizer(text, return_tensors=\"pt\")\n",
        "roberta_outputs = roberta_model(**roberta_inputs)\n",
        "roberta_prediction = roberta_outputs.logits.argmax(dim=1).item()\n",
        "roberta_sentiment = roberta_config.id2label[roberta_prediction]\n",
        "\n",
        "print(text)\n",
        "print(f\"Finbert Sentiment Prediction: {finbert_sentiment} ({sentiment_names.index(finbert_sentiment)})\")\n",
        "print(f\"Roberta Sentiment Prediction: {roberta_sentiment} ({sentiment_names.index(roberta_sentiment)})\")"
      ],
      "metadata": {
        "id": "qVOO1HNamEBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make this a more generic inference function so we can run it in a generic inference loop later."
      ],
      "metadata": {
        "id": "C4bObWvFr17d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_encoder_infer(model_name: str) -> Callable[[str], int]:\n",
        "    \"\"\"\n",
        "    Generates a model_infer function for a given encoder model.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model to use for inference.\n",
        "\n",
        "    Returns:\n",
        "        A model_infer function that takes a sentence as input and returns a predicted label as an integer.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # REPLACE None WITH YOUR CODE\n",
        "\n",
        "    # Initialize the model\n",
        "    tokenizer = None\n",
        "    config = None\n",
        "    model = None\n",
        "\n",
        "    def model_infer(text: str) -> int:\n",
        "        \"\"\"\n",
        "        Runs inference on a encoder model and classifies the classification of a given text.\n",
        "\n",
        "        Args:\n",
        "            text: The sentence to classify.\n",
        "\n",
        "        Returns:\n",
        "            The predicted label as an integer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Run inference\n",
        "        inputs = None\n",
        "        outputs = None\n",
        "        prediction = None\n",
        "        sentiment = None\n",
        "\n",
        "        # Finally, we will get the sentiment index associated with this sentiment\n",
        "        # You might be wondering why we don't just use the argmax over the output logits!\n",
        "        # This is because the model might have used different indices for its labels.\n",
        "        sentiment_index = None\n",
        "\n",
        "        #########################\n",
        "        # FINISH YOUR CODE HERE #\n",
        "        #########################\n",
        "\n",
        "        return sentiment_index\n",
        "\n",
        "    return model_infer\n",
        "\n",
        "finbert_infer = generate_encoder_infer(\"ProsusAI/finbert\")\n",
        "roberta_infer = generate_encoder_infer(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
      ],
      "metadata": {
        "id": "eOJBh39tr7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Your Code!\n",
        "class TestGenerateEncoderInfer(unittest.TestCase):\n",
        "\n",
        "    def test_generate_encoder_infer_finbert(self):\n",
        "        \"\"\"\n",
        "        Test generate_encoder_infer with a mock for Finbert model.\n",
        "        \"\"\"\n",
        "        model_infer = generate_encoder_infer(\"ProsusAI/finbert\")\n",
        "\n",
        "        # Mocked behavior for this test\n",
        "        text = \"Revenue grew 12 percent to (  x20ac ) 3.6 billion ( US$ 4.5 billion ) .\"\n",
        "        expected_output = 2  # 'positive' in the sentiment_names list\n",
        "\n",
        "        # Running the model inference function\n",
        "        result = model_infer(text)\n",
        "\n",
        "        # Assert that the result is as expected\n",
        "        self.assertEqual(result, expected_output)\n",
        "\n",
        "    def test_generate_encoder_infer_roberta(self):\n",
        "        \"\"\"\n",
        "        Test generate_encoder_infer with a mock for Roberta model.\n",
        "        \"\"\"\n",
        "        model_infer = generate_encoder_infer(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "        # Mocked behavior for this test\n",
        "        text = \"Revenue grew 12 percent to (  x20ac ) 3.6 billion ( US$ 4.5 billion ) .\"\n",
        "        expected_output = 2  # 'positive' in the sentiment_names list\n",
        "\n",
        "        # Running the model inference function\n",
        "        result = model_infer(text)\n",
        "\n",
        "        # Assert that the result is as expected\n",
        "        self.assertEqual(result, expected_output)\n",
        "\n",
        "# Run the updated tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestGenerateEncoderInfer))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jGlJZdAHHHWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Inference\n",
        "\n",
        "Now let's do the same for the decoder model: Phi-1.5.  First, we'll provide some example code that demonstrates the use and versatility of a decoder model."
      ],
      "metadata": {
        "id": "UZGQ9ZB2nXWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, load the Tokenizer and Model, just like with FinBert and RoBerta\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)"
      ],
      "metadata": {
        "id": "2y58QJJuqkOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first show the model's versatility.\n",
        "# We're going to have it write some code and then run the code!\n",
        "# This is an example from the model card on huggingface: https://huggingface.co/microsoft/phi-1_5\n",
        "# Note that the model card explains that the model is not fine-tuned, so it tends to ramble and continue generating text after it answers the question.\n",
        "# We'll use some very basic filtering to extract exactly what we want from the generated output\n",
        "inputs = tokenizer('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "start_index = text.find(\"```python\")\n",
        "end_index = text.find(\"```\", start_index + 3)\n",
        "generated_code = text[start_index + 9:end_index]\n",
        "\n",
        "print(\"This is the code that the model generated:\")\n",
        "print(\"---------\")\n",
        "print(generated_code)\n",
        "print(\"---------\")\n",
        "print(\"Now, let's execute that code:\")\n",
        "exec(generated_code)"
      ],
      "metadata": {
        "id": "6MFMK09UWI1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's hard to really comprehend how amazing this last code cell is.  This model is reliably generating executable, syntax compliant, python code.  Decoder models have incredible versatility- maybe this model could even help you with this coding project!\n",
        "\n",
        "Now we're going to ask the same model to do an entirely different task: sentiment analysis.  Let's check it out!"
      ],
      "metadata": {
        "id": "8M1JG8S3YUNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, try it out for classifying a zero-shot prompt!\n",
        "zero_shot_generator = generate_n_shot_examples(\"Classify the sentiment (negative, neutral, positive) of the following statement\",\n",
        "                                               phrasebank_split['train'], phrasebank_split['test'], 0)\n",
        "zero_shot_sample = next(zero_shot_generator)\n",
        "\n",
        "\n",
        "zero_shot_prompt = zero_shot_sample[0]\n",
        "zero_shot_label = zero_shot_sample[1]\n",
        "# zero_shot_inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "# input_length = zero_shot_inputs.input_ids.shape[1]\n",
        "# zero_shot_outputs = model.generate(**zero_shot_inputs, max_new_tokens=20, return_dict_in_generate=True, pad_token_id=tokenizer.eos_token_id)\n",
        "# zero_shot_output_tokens = zero_shot_outputs.sequences[0, input_length:]\n",
        "# zero_shot_output_str = tokenizer.decode(zero_shot_output_tokens)\n",
        "\n",
        "print(zero_shot_prompt)\n",
        "print('-----------')\n",
        "print(f'True label: {zero_shot_label}')\n",
        "print('-----------')\n",
        "print('(generating...)')\n",
        "zero_shot_inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "input_len = zero_shot_inputs.input_ids.shape[1]\n",
        "zero_shot_outputs = model.generate(**zero_shot_inputs, max_length=200)\n",
        "zero_shot_output_str = tokenizer.batch_decode(zero_shot_outputs[:,input_len:])[0]\n",
        "print('-----------')\n",
        "print(f'Decoder output (cleaned): {zero_shot_output_str.strip().split()[0]}')\n",
        "print('-----------')\n",
        "print(f'Decoder output (raw): {zero_shot_output_str}')"
      ],
      "metadata": {
        "id": "A-wYYm-KnjmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, try it out for classifying a one-shot prompt!\n",
        "one_shot_generator = generate_n_shot_examples(\"Classify the sentiment (negative, neutral, positive) of the following statement\",\n",
        "                                               phrasebank_split['train'], phrasebank_split['test'], 1)\n",
        "one_shot_sample = next(one_shot_generator)\n",
        "\n",
        "\n",
        "one_shot_prompt = one_shot_sample[0]\n",
        "one_shot_label = one_shot_sample[1]\n",
        "\n",
        "print(one_shot_prompt)\n",
        "print('-----------')\n",
        "print(f'True label: {one_shot_label}')\n",
        "print('-----------')\n",
        "print('(generating...)')\n",
        "one_shot_inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "input_len = one_shot_inputs.input_ids.shape[1]\n",
        "one_shot_outputs = model.generate(**one_shot_inputs, max_length=200)\n",
        "one_shot_output_str = tokenizer.batch_decode(one_shot_outputs[:,input_len:])[0]\n",
        "print('-----------')\n",
        "print(f'Decoder output (cleaned): {one_shot_output_str.strip().split()[0]}')\n",
        "print('-----------')\n",
        "print(f'Decoder output (raw): {one_shot_output_str}')"
      ],
      "metadata": {
        "id": "GVypVzLEnjnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, try it out for classifying a ten-shot prompt!\n",
        "ten_shot_generator = generate_n_shot_examples(\"Classify the sentiment (negative, neutral, positive) of the following statement\",\n",
        "                                               phrasebank_split['train'], phrasebank_split['test'], 10)\n",
        "ten_shot_sample = next(ten_shot_generator)\n",
        "\n",
        "\n",
        "ten_shot_prompt = ten_shot_sample[0]\n",
        "ten_shot_label = ten_shot_sample[1]\n",
        "\n",
        "print(ten_shot_prompt)\n",
        "print('-----------')\n",
        "print(f'True label: {ten_shot_label}')\n",
        "print('-----------')\n",
        "print('(generating...)')\n",
        "ten_shot_inputs = tokenizer(ten_shot_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "input_len = ten_shot_inputs.input_ids.shape[1]\n",
        "ten_shot_outputs = model.generate(**ten_shot_inputs, max_length=200)\n",
        "ten_shot_output_str = tokenizer.batch_decode(ten_shot_outputs[:,input_len:])[0]\n",
        "print('-----------')\n",
        "print(f'Decoder output (cleaned): {ten_shot_output_str.strip().split()[0]}')\n",
        "print('-----------')\n",
        "print(f'Decoder output (raw): {ten_shot_output_str}')"
      ],
      "metadata": {
        "id": "Yo67upisnjpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model            # Delete the model variable\n",
        "torch.cuda.empty_cache()  # Empty CUDA cache to release GPU memory"
      ],
      "metadata": {
        "id": "T29gEKirnWX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've played around a little with Phi-1.5 inference, let'\n",
        "s build an inference function that is similar to the one we built earlier for the encoder models."
      ],
      "metadata": {
        "id": "2ckVz3uhrz81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_decoder_infer(model_name: str) -> Callable[[str], int]:\n",
        "    \"\"\"\n",
        "    Generates a model_infer function for a given decoder model.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the model to use for inference.\n",
        "\n",
        "    Returns:\n",
        "        A model_infer function that takes a sentence as input and returns a predicted label as an integer.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # REPLACE None WITH YOUR CODE\n",
        "\n",
        "    # Initialize the model\n",
        "    tokenizer = None\n",
        "    model = None\n",
        "\n",
        "    def model_infer(text: str, max_new_tokens: int = 20) -> int:\n",
        "        \"\"\"\n",
        "        Runs inference on a decoder model and classifies the classification of a given text.\n",
        "\n",
        "        Args:\n",
        "            text: The sentence to classify.\n",
        "\n",
        "        Returns:\n",
        "            The predicted label as an integer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Run inference\n",
        "        inputs = None\n",
        "        input_len = None\n",
        "        outputs = None\n",
        "        output_str = None\n",
        "        output_str_cleaned = None\n",
        "\n",
        "        return output_str_cleaned\n",
        "\n",
        "        #########################\n",
        "        # FINISH YOUR CODE HERE #\n",
        "        #########################\n",
        "\n",
        "    return model_infer\n",
        "\n",
        "decoder_infer = generate_decoder_infer(\"microsoft/phi-1_5\")"
      ],
      "metadata": {
        "id": "uG_Kc5E-rzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Your Code!\n",
        "class TestGenerateDecoderInfer(unittest.TestCase):\n",
        "\n",
        "    def test_generate_decoder_infer_1(self):\n",
        "        self.assertEqual(decoder_infer(\"one\"), \"_hot(labels,\")\n",
        "\n",
        "    def test_generate_decoder_infer_2(self):\n",
        "        self.assertEqual(decoder_infer(\"one two\"), \"-dimensional\")\n",
        "\n",
        "    def test_generate_decoder_infer_3(self):\n",
        "        self.assertEqual(decoder_infer(\"one two three four\"), \"-legged\")\n",
        "\n",
        "# Run the updated tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestGenerateDecoderInfer))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jPoEEUpLnjro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Now, let's evaluate how well our models do on the dataset!  We'll start with the encoder models because those are simple.  We just infer on each test example and check if the output matches!"
      ],
      "metadata": {
        "id": "GHZtA_SieFuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_encoder_inference(model_infer: Callable[[str], int], test_dataset: iter) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a model on a test dataset, with progress bar.\n",
        "\n",
        "    Args:\n",
        "        model_infer: The model inference function that takes text as input and returns an integer label.\n",
        "        test_dataset: A generator or iterable containing test samples. Each sample is a dict with keys \"label\" and \"sentence\".\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the average accuracy and F1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # REPLACE None WITH YOUR CODE\n",
        "\n",
        "    # Initialize true and predicted labels to empty Python lists\n",
        "    true_labels = None\n",
        "    predicted_labels = None\n",
        "\n",
        "    for sample in tqdm(test_dataset, total=len(test_dataset), desc=\"Evaluating\"):\n",
        "        # Retrieve the 'label' and 'sentence' from the sample dict\n",
        "        true_label = None\n",
        "        sentence = None\n",
        "\n",
        "        # Run model inference to calculate the predicted_label\n",
        "        predicted_label = None\n",
        "\n",
        "        # Add the true and predicted labels to the true and predicted labels arrays\n",
        "        true_labels.append(None)\n",
        "        predicted_labels.append(None)\n",
        "\n",
        "    # calcualte accuracy and F1 score using sklearn's accuracy_score() and f1_score() functions\n",
        "    avg_accuracy = None\n",
        "    avg_f1 = None\n",
        "\n",
        "    #########################\n",
        "    # FINISH YOUR CODE HERE #\n",
        "    #########################\n",
        "\n",
        "    return {'Average Accuracy': avg_accuracy, 'Average F1 Score': avg_f1}"
      ],
      "metadata": {
        "id": "K-0Ayj0leW09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Your Code!\n",
        "class TestEvaluateEncoderInferenceExtended(unittest.TestCase):\n",
        "\n",
        "    def test_all_correct_predictions(self):\n",
        "        \"\"\"\n",
        "        Test when all predictions are correct.\n",
        "        \"\"\"\n",
        "        mock_model_infer = Mock()\n",
        "        mock_model_infer.side_effect = [0, 1, 2, 1, 0]  # All predictions are correct\n",
        "\n",
        "        mock_test_dataset = [\n",
        "            {'label': 0, 'sentence': 'This is bad.'},\n",
        "            {'label': 1, 'sentence': 'This is okay.'},\n",
        "            {'label': 2, 'sentence': 'This is great.'},\n",
        "            {'label': 1, 'sentence': 'This is fine.'},\n",
        "            {'label': 0, 'sentence': 'This is terrible.'}\n",
        "        ]\n",
        "\n",
        "        result = evaluate_encoder_inference(mock_model_infer, mock_test_dataset)\n",
        "        self.assertEqual(result['Average Accuracy'], 1.0)\n",
        "        self.assertEqual(result['Average F1 Score'], 1.0)\n",
        "\n",
        "    def test_some_incorrect_predictions(self):\n",
        "        \"\"\"\n",
        "        Test when some predictions are incorrect.\n",
        "        \"\"\"\n",
        "        mock_model_infer = Mock()\n",
        "        mock_model_infer.side_effect = [0, 1, 0, 1, 2]  # Some predictions are incorrect\n",
        "\n",
        "        mock_test_dataset = [\n",
        "            {'label': 0, 'sentence': 'This is bad.'},\n",
        "            {'label': 1, 'sentence': 'This is okay.'},\n",
        "            {'label': 2, 'sentence': 'This is great.'},\n",
        "            {'label': 1, 'sentence': 'This is fine.'},\n",
        "            {'label': 0, 'sentence': 'This is terrible.'}\n",
        "        ]\n",
        "\n",
        "        result = evaluate_encoder_inference(mock_model_infer, mock_test_dataset)\n",
        "        self.assertLess(result['Average Accuracy'], 1.0)\n",
        "        self.assertLess(result['Average F1 Score'], 1.0)\n",
        "\n",
        "    def test_all_incorrect_predictions(self):\n",
        "        \"\"\"\n",
        "        Test when all predictions are incorrect.\n",
        "        \"\"\"\n",
        "        mock_model_infer = Mock()\n",
        "        mock_model_infer.side_effect = [2, 2, 2, 2, 2]  # All predictions are incorrect\n",
        "\n",
        "        mock_test_dataset = [\n",
        "            {'label': 0, 'sentence': 'This is bad.'},\n",
        "            {'label': 1, 'sentence': 'This is okay.'},\n",
        "            {'label': 0, 'sentence': 'This is terrible.'},\n",
        "            {'label': 1, 'sentence': 'This is not good.'},\n",
        "            {'label': 0, 'sentence': 'This is awful.'}\n",
        "        ]\n",
        "\n",
        "        result = evaluate_encoder_inference(mock_model_infer, mock_test_dataset)\n",
        "        self.assertEqual(result['Average Accuracy'], 0.0)\n",
        "        self.assertEqual(result['Average F1 Score'], 0.0)\n",
        "\n",
        "# Run the extended test suite\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestEvaluateEncoderInferenceExtended))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0das-vNh6dj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate the evaluation metrics (accuracy and F1 score) for Roberta and Finbert!\n",
        "roberta_results = evaluate_encoder_inference(roberta_infer, phrasebank_split['test'])\n",
        "finbert_results = evaluate_encoder_inference(finbert_infer, phrasebank_split['test'])"
      ],
      "metadata": {
        "id": "pvYlsArMnLSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the ASCII table\n",
        "header = \"| Model Name  | Average Accuracy | Average F1 Score |\"\n",
        "separator = \"-\" * len(header)\n",
        "\n",
        "# Print the ASCII table\n",
        "print(separator)\n",
        "print(header)\n",
        "print(separator)\n",
        "print(f\"|   Roberta   | {roberta_results['Average Accuracy']:<16.4f} | {roberta_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|   Finbert   | {finbert_results['Average Accuracy']:<16.4f} | {finbert_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)"
      ],
      "metadata": {
        "id": "H46pt6mumfgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's do the same for the decoder model!  It's a little more complex because we need to generate the n-shot examples first."
      ],
      "metadata": {
        "id": "0yY1E3BQo4Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_decoder_inference(model_infer: Callable[[str], int], test_dataset: iter, train_set: List[Dict[str, Any]], n: int, prompt: str) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a decoder model on a test dataset, with progress bar and n-shot learning.\n",
        "\n",
        "    Args:\n",
        "        model_infer: The model inference function that takes text as input and returns an integer label.\n",
        "        test_dataset: A generator or iterable containing test samples. Each sample is a dict with keys \"label\" and \"sentence\".\n",
        "        train_set: The training set used for generating n-shot examples.\n",
        "        n: The number of training examples to use for n-shot learning.\n",
        "        prompt: The prompt text to be used for the n-shot examples.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the average accuracy and F1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################\n",
        "    # START YOUR CODE HERE #\n",
        "    ########################\n",
        "    # REPLACE None WITH YOUR CODE\n",
        "\n",
        "    # Initialize true and predicted labels to empty Python lists\n",
        "    true_labels = None\n",
        "    predicted_labels = None\n",
        "\n",
        "    # This time, we need to use our generator function\n",
        "    # generate_n_shot_examples()\n",
        "    # Loaded with the prompt, train and test datasets, and n\n",
        "    n_shot_gen = None\n",
        "\n",
        "    for input_str, true_label in tqdm(n_shot_gen, total=len(test_dataset), desc=\"Evaluating\"):\n",
        "        # Calculate the predicted label from the n_shot_example\n",
        "        predicted_label = None\n",
        "\n",
        "        # Add the true and predicted labels to the true and predicted labels arrays\n",
        "        true_labels.append(None)\n",
        "        predicted_labels.append(None)\n",
        "\n",
        "    # Calculate accuracy and F1 score using sklearn's accuracy_score() and f1_score() functions\n",
        "    avg_accuracy = None\n",
        "    avg_f1 = None\n",
        "\n",
        "    #########################\n",
        "    # FINISH YOUR CODE HERE #\n",
        "    #########################\n",
        "\n",
        "    return {'Average Accuracy': avg_accuracy, 'Average F1 Score': avg_f1}"
      ],
      "metadata": {
        "id": "c-KLFxz2oSJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Your Code!\n",
        "class TestEvaluateDecoderInference(unittest.TestCase):\n",
        "\n",
        "    def test_all_correct_predictions(self):\n",
        "        \"\"\"\n",
        "        Test when all predictions are correct.\n",
        "        \"\"\"\n",
        "        mock_model_infer = Mock()\n",
        "        mock_model_infer.side_effect = ['negative', 'neutral', 'positive', 'neutral', 'positive']  # All predictions are correct\n",
        "\n",
        "        mock_test_dataset = [\n",
        "            {'label': 0, 'sentence': 'This is bad.'},\n",
        "            {'label': 1, 'sentence': 'This is okay.'},\n",
        "            {'label': 2, 'sentence': 'This is great.'},\n",
        "            {'label': 1, 'sentence': 'This is fine.'},\n",
        "            {'label': 0, 'sentence': 'This is terrible.'}\n",
        "        ]\n",
        "\n",
        "        mock_train_set = [\n",
        "            {'label': 0, 'sentence': 'Not good'},\n",
        "            {'label': 1, 'sentence': 'So so'},\n",
        "            {'label': 2, 'sentence': 'Excellent'},\n",
        "            {'label': 1, 'sentence': 'Average'},\n",
        "            {'label': 0, 'sentence': 'Bad'}\n",
        "        ]\n",
        "\n",
        "        prompt = \"Classify the sentiment (negative, neutral, positive) of the following statement\"\n",
        "\n",
        "        result = evaluate_decoder_inference(mock_model_infer, mock_test_dataset, mock_train_set, 2, prompt)\n",
        "        self.assertEqual(result['Average Accuracy'], 1.0)\n",
        "        self.assertEqual(result['Average F1 Score'], 1.0)\n",
        "\n",
        "    def test_some_incorrect_predictions(self):\n",
        "        \"\"\"\n",
        "        Test when some predictions are incorrect.\n",
        "        \"\"\"\n",
        "        mock_model_infer = Mock()\n",
        "        mock_model_infer.side_effect = ['negative', 'neutral', 'negative', 'neutral', 'positive']  # Some predictions are incorrect\n",
        "\n",
        "        mock_test_dataset = [\n",
        "            {'label': 0, 'sentence': 'This is bad.'},\n",
        "            {'label': 1, 'sentence': 'This is okay.'},\n",
        "            {'label': 2, 'sentence': 'This is great.'},\n",
        "            {'label': 1, 'sentence': 'This is fine.'},\n",
        "            {'label': 0, 'sentence': 'This is terrible.'}\n",
        "        ]\n",
        "\n",
        "        mock_train_set = [\n",
        "            {'label': 0, 'sentence': 'Not good'},\n",
        "            {'label': 1, 'sentence': 'So so'},\n",
        "            {'label': 2, 'sentence': 'Excellent'},\n",
        "            {'label': 1, 'sentence': 'Average'},\n",
        "            {'label': 0, 'sentence': 'Bad'},\n",
        "            {'label': 0, 'sentence': 'Not good'},\n",
        "            {'label': 1, 'sentence': 'So so'},\n",
        "            {'label': 2, 'sentence': 'Excellent'},\n",
        "            {'label': 1, 'sentence': 'Average'},\n",
        "            {'label': 0, 'sentence': 'Bad'}\n",
        "        ]\n",
        "\n",
        "        prompt = \"Classify the sentiment (negative, neutral, positive) of the following statement\"\n",
        "\n",
        "        result = evaluate_decoder_inference(mock_model_infer, mock_test_dataset, mock_train_set, 2, prompt)\n",
        "        self.assertLess(result['Average Accuracy'], 1.0)\n",
        "        self.assertLess(result['Average F1 Score'], 1.0)\n",
        "\n",
        "# Run the extended test suite for evaluate_decoder_inference\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestEvaluateDecoderInference))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K8O_7gSA-lcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate the evaluation metrics (accuracy and F1 score) for Phi-1.5 (0-shot, 1-shot, 2-shot, and 3-shot!\n",
        "# Increasing the size of the test set will increase the time required for inference.\n",
        "num_test_samples = 100\n",
        "decoder_zero_results = evaluate_decoder_inference(decoder_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 0, \"Classify the sentiment (negative, neutral, positive) of the following statement\")\n",
        "decoder_one_results = evaluate_decoder_inference(decoder_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 1, \"Classify the sentiment (negative, neutral, positive) of the following statement\")\n",
        "decoder_two_results = evaluate_decoder_inference(decoder_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 2, \"Classify the sentiment (negative, neutral, positive) of the following statement\")\n",
        "decoder_three_results = evaluate_decoder_inference(decoder_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 3, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "OT-jO52au6y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the ASCII table\n",
        "header = \"| Model Name  | Average Accuracy | Average F1 Score |\"\n",
        "separator = \"-\" * len(header)\n",
        "\n",
        "# Print the ASCII table\n",
        "print(separator)\n",
        "print(header)\n",
        "print(separator)\n",
        "print(f\"| Decoder (0-shot)| {decoder_zero_results['Average Accuracy']:<16.4f} | {decoder_zero_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"| Decoder (1-shot)| {decoder_one_results['Average Accuracy']:<16.4f} | {decoder_one_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"| Decoder (2-shot)| {decoder_two_results['Average Accuracy']:<16.4f} | {decoder_two_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"| Decoder (3-shot)| {decoder_three_results['Average Accuracy']:<16.4f} | {decoder_three_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)"
      ],
      "metadata": {
        "id": "BMiM7PAKoSLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "n_values = [0, 1, 2, 3]  # n-values for Phi-1.5\n",
        "decoder_accuracies = [\n",
        "    decoder_zero_results['Average Accuracy'],\n",
        "    decoder_one_results['Average Accuracy'],\n",
        "    decoder_two_results['Average Accuracy'],\n",
        "    decoder_three_results['Average Accuracy']\n",
        "]\n",
        "roberta_accuracy = roberta_results['Average Accuracy']\n",
        "finbert_accuracy = finbert_results['Average Accuracy']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot Phi-1.5 accuracies\n",
        "plt.plot(n_values, decoder_accuracies, marker='o', linestyle='-', label='Phi-1.5', color='b')\n",
        "\n",
        "# Plot horizontal lines for Roberta and Finbert\n",
        "plt.axhline(y=roberta_accuracy, color='r', linestyle='--', label='Roberta')\n",
        "plt.axhline(y=finbert_accuracy, color='g', linestyle='--', label='Finbert')\n",
        "\n",
        "# Adding labels, title, and legend\n",
        "plt.xlabel('Number of Shots (n)')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Model Accuracy Comparison with n-shot Learning')\n",
        "plt.xticks(np.arange(min(n_values), max(n_values)+1, 1))\n",
        "plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ryfr4bumoSPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Results and Discussion\n",
        "\n",
        "## Key Insights:\n",
        "\n",
        "1. **Finbert's Strength**: Among the encoder models, Finbert substantially outperformed Roberta. This could be due to Finbert's specialized training on financial text, making it particularly well-suited for our dataset.\n",
        "\n",
        "2. **Adaptability of Decoder Models**: Phi-1.5 performed poorly in a zero-shot setting but showed a remarkable ability to adapt and improve its performance with just a single example (one-shot).\n",
        "\n",
        "3. **Diminishing Returns**: For Phi-1.5, we observed that adding more examples (two-shot and three-shot) did not significantly improve the performance over the one-shot case.\n",
        "\n",
        "4. **Specialization vs. Flexibility**: While Finbert showed excellent performance, it's specialized for financial text sentiment analysis. Phi-1.5, on the other hand, demonstrated the capability to adapt to the task with minimal examples.\n",
        "\n",
        "This evaluation showcases the strengths and weaknesses of both encoder and decoder models in a few-shot learning scenario and provides valuable insights into choosing the right model for your NLP tasks.\n"
      ],
      "metadata": {
        "id": "waUUepAD4tYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLEASE NOTE THAT THE FOLLOWING SECTION IS OPTIONAL AND WILL COST UNDER $1 IN OPENAI CREDITS**\n",
        "\n",
        "# OpenAI API (OPTIONAL)\n",
        "\n",
        "We will explore using OpenAI's GPT-3.5 model to perform the same tasks as the open source HuggingFace models above.  Once you have signed up for OpenAI and generated an API secret key, continue with the rest fo the assignment!"
      ],
      "metadata": {
        "id": "MdLFneGImhZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenAI API client\n",
        "if openai.api_key is None:\n",
        "    # Prompt for the API key\n",
        "    api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "    # Assign the API key to your application-specific variable\n",
        "    openai.api_key = api_key"
      ],
      "metadata": {
        "id": "HtDMbgJimgfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before working on the next section of code, please take a minute to play with the OpenAI chat Playground.  We will be using the GPT-3.5 Turbo Chat API.  Once you have interacted a little with the model, click the View Code button to see how it works!  Try it with one of the example few-shot samples from earlier like this:\n",
        "\n",
        "![Image description](https://mrplantsdropbox.blob.core.windows.net/dropbox/openai_screenshot.jpg)\n",
        "\n",
        "Now that you've had a chance to practice with a GUI version of the API, let's create an inference function for OpenAI's GPT-3.5 Turbo."
      ],
      "metadata": {
        "id": "yWoJxUmLSdIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "def create_gpt3_infer(model_name: str) -> Callable[[str], str]:\n",
        "    \"\"\"\n",
        "    Creates an inference function for a GPT-3.5 Turbo model.\n",
        "\n",
        "    Args:\n",
        "        model_name: The name of the GPT-3.5 Turbo model to use for inference.\n",
        "\n",
        "    Returns:\n",
        "        A function that takes a sentence as input and returns a predicted label as an integer.\n",
        "    \"\"\"\n",
        "\n",
        "    def gpt3_infer(text: str) -> str:\n",
        "        \"\"\"\n",
        "        Runs inference on a specified GPT-3.5 Turbo model via the OpenAI API\n",
        "        and classifies the sentiment of a given text.\n",
        "\n",
        "        The function communicates with the GPT-3.5 Turbo model via the OpenAI API and\n",
        "        obtains a response based on the input text. The model may return out-of-range\n",
        "        responses (i.e., not 0, 1, or 2), in which case this function returns -1.\n",
        "\n",
        "        Args:\n",
        "            text: The sentence to classify.\n",
        "\n",
        "        Returns:\n",
        "            The predicted label as an string.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################\n",
        "        # START YOUR CODE HERE #\n",
        "        ########################\n",
        "        # REPLACE None WITH YOUR CODE\n",
        "\n",
        "        # Make an API call to OpenAI for inference\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model_name,\n",
        "            messages=None\n",
        "            temperature=1,\n",
        "            max_tokens=256,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "        )\n",
        "\n",
        "        # Extract the response content from the API response\n",
        "        response_content = None\n",
        "\n",
        "        #########################\n",
        "        # FINISH YOUR CODE HERE #\n",
        "        #########################\n",
        "\n",
        "        return response_content\n",
        "\n",
        "    return gpt3_infer\n",
        "\n",
        "# Initialize the GPT-3.5 Turbo inference function with a model name\n",
        "gpt3_infer = create_gpt3_infer(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "xa_KGGtjJgEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Your Code!\n",
        "class TestCreateGpt3InferIntegration(unittest.TestCase):\n",
        "\n",
        "    def test_create_gpt3_infer_example_sentence(self):\n",
        "        \"\"\"\n",
        "        Integration test for create_gpt3_infer using a provided example sentence.\n",
        "        Calls the API multiple times and takes the plurality result.\n",
        "        \"\"\"\n",
        "        # Initialize the inference function\n",
        "        gpt3_infer = create_gpt3_infer(\"gpt-3.5-turbo\")\n",
        "\n",
        "        # Provided example text\n",
        "        text = '''Classify the sentiment (negative, neutral, positive) of the following statement:\n",
        "        [Finnish meat company Atria can no longer promise a sufficient amount of domestic beef to its customers .]\n",
        "        Reponse: negative\n",
        "        [Finnish Raisio ( Diagnostics ) is launching new DNA-based quick tests to ensure the safety of food .]\n",
        "        Reponse: positive\n",
        "        [Export declined by 6 percent to 16.4 million liters]\n",
        "        Reponse: '''\n",
        "\n",
        "        # Run the inference function multiple times and collect results\n",
        "        num_trials = 10  # Number of times to call the API\n",
        "        results = [gpt3_infer(text) for _ in range(num_trials)]\n",
        "\n",
        "        # Find the most common result\n",
        "        counter = Counter(results)\n",
        "        most_common_result, _ = counter.most_common(1)[0]\n",
        "\n",
        "        # Check that the most common result is negative\n",
        "        self.assertEqual(most_common_result, 'negative')\n",
        "\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestCreateGpt3InferIntegration))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "meL5CxWQYC67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the network connection will slow down these requests\n",
        "# To save time, let's only look at 20 of the test samples.\n",
        "# You can increase this to get a more precise value for the model's test set accuracy\n",
        "num_test_samples = 50"
      ],
      "metadata": {
        "id": "eUJzdKvwQrEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_0shot_results = evaluate_decoder_inference(gpt3_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 0, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "hgovWXjjo-5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_1shot_results = evaluate_decoder_inference(gpt3_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 1, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "74YVfxmpzqfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_2shot_results = evaluate_decoder_inference(gpt3_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 2, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "qUiXYd4Yzqrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_3shot_results = evaluate_decoder_inference(gpt3_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 3, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "vCTVDgyRzqw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the ASCII table\n",
        "header = \"| Model Name  | Average Accuracy | Average F1 Score |\"\n",
        "separator = \"-\" * len(header)\n",
        "\n",
        "# Print the ASCII table\n",
        "print(separator)\n",
        "print(header)\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 (n=0)| {gpt3_0shot_results['Average Accuracy']:<16.4f} | {gpt3_0shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 (n=1)| {gpt3_1shot_results['Average Accuracy']:<16.4f} | {gpt3_1shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 (n=2)| {gpt3_2shot_results['Average Accuracy']:<16.4f} | {gpt3_2shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 (n=3)| {gpt3_3shot_results['Average Accuracy']:<16.4f} | {gpt3_3shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)"
      ],
      "metadata": {
        "id": "k2x-n3oHqQiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "n_values = [0, 1, 2, 3]  # n-values for GPT-3.5\n",
        "gpt3_accuracies = [\n",
        "    gpt3_0shot_results['Average Accuracy'],\n",
        "    gpt3_1shot_results['Average Accuracy'],\n",
        "    gpt3_2shot_results['Average Accuracy'],\n",
        "    gpt3_3shot_results['Average Accuracy']\n",
        "]\n",
        "roberta_accuracy = roberta_results['Average Accuracy']\n",
        "finbert_accuracy = finbert_results['Average Accuracy']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot GPT-3.5 accuracies\n",
        "plt.plot(n_values, gpt3_accuracies, marker='o', linestyle='-', label='GPT-3.5', color='orange')\n",
        "\n",
        "# Plot Phi-1.5 accuracies\n",
        "plt.plot(n_values, decoder_accuracies, marker='o', linestyle='-', label='Phi-1.5', color='b')\n",
        "\n",
        "# Plot horizontal lines for Roberta and Finbert\n",
        "plt.axhline(y=roberta_accuracy, color='r', linestyle='--', label='Roberta')\n",
        "plt.axhline(y=finbert_accuracy, color='g', linestyle='--', label='Finbert')\n",
        "\n",
        "# Adding labels, title, and legend\n",
        "plt.xlabel('Number of Shots (n)')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Model Accuracy Comparison with n-shot Learning')\n",
        "plt.xticks(np.arange(min(n_values), max(n_values)+1, 1))\n",
        "plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oPW_u-7Kz5cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning with GPT-3.5 Turbo (OPTIONAL)\n",
        "\n",
        "**PLEASE NOTE THAT THIS OPTIONAL FINE TUNING SECTION WILL COST AROUND $7 IN OPENAI CREDITS**\n",
        "\n",
        "Note also that this section does not have any active coding exercises.  The cells will all run as-written and showcase how to fine-tune using the OpenAI API.\n",
        "\n",
        "In the previous sections, we explored how different models perform on sentiment classification tasks, using both encoder models like Roberta and Finbert as well as decoder models like GPT-3.5 Turbo. We also investigated the impact of n-shot learning on the performance of these models.\n",
        "\n",
        "Now, we're going to take it a step further by fine-tuning GPT-3.5 Turbo on our specific dataset. Fine-tuning allows us to adapt a pre-trained model to better perform on the task at hand, potentially increasing its accuracy and F1 score.\n",
        "\n",
        "In this section, you'll learn how to:\n",
        "\n",
        "1. Prepare your dataset in a format suitable for fine-tuning with OpenAI's API.\n",
        "2. Upload this dataset to OpenAI.\n",
        "3. Initiate and monitor the fine-tuning process.\n",
        "\n",
        "This will give you a comprehensive understanding of how fine-tuning can affect model performance and when it might be advantageous to use it.\n",
        "\n",
        "Before getting started, take a look at [OpenAI's fine-tuning documentation](https://platform.openai.com/docs/guides/fine-tuning), which describes the steps to fine-tuning their models.  We will focus on fine-tuning the GPT-3.5 Turbo model.\n"
      ],
      "metadata": {
        "id": "IMdtYxEvRAH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset for OpenAI API\n",
        "OpenAI accepts fine-tuning data as jsonl files (a format that lists a json object on each line).  We will prepare such a file with zero-shot examples only, to save on tokens."
      ],
      "metadata": {
        "id": "PFmYxrITUJwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_openai_conversation(sample: dict) -> dict:\n",
        "    \"\"\"Converts a sample to the OpenAI conversation format.\n",
        "\n",
        "    Args:\n",
        "        sample: A dictionary containing 'sentence' and 'label'.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary in the OpenAI conversation format.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": sample['sentence']},\n",
        "            {\"role\": \"assistant\", \"content\": str(sample['label'])}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def write_training_file_to_disk(samples: iter, filename: str) -> None:\n",
        "    \"\"\"Writes the training samples to a file in OpenAI conversation format.\n",
        "\n",
        "    Args:\n",
        "        samples: An iterator of samples.\n",
        "        filename: The name of the file to write the samples to.\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for sample in samples:\n",
        "            conversation = create_openai_conversation(sample)\n",
        "            f.write(json.dumps(conversation) + '\\n')\n",
        "\n",
        "# Specify the number of samples per category\n",
        "sample_count_per_category = 100\n",
        "\n",
        "# Specify the path for the output JSONL file\n",
        "file_path = 'train.jsonl'\n",
        "\n",
        "# Generate the JSONL file\n",
        "write_training_file_to_disk(phrasebank_split['train'], file_path)"
      ],
      "metadata": {
        "id": "Uz1kNA0eSRJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiate Fine Tuning\n",
        "Now that the training file is ready, it's time to upload the data and then initiate a training job.  Once you have the training job initiated, you can monitor its progress using the openai API like this:\n",
        "\n",
        "```python\n",
        "# List the most recent 10 events from a fine-tuning job\n",
        "openai.FineTuningJob.list_events(id=fine_tune_response['id'], limit=10)\n",
        "```\n",
        "Or you can watch the progress using the [OpenAI web portal](https://platform.openai.com/finetune).\n",
        "\n",
        "**Note that it could take over an hour to fine-tune**"
      ],
      "metadata": {
        "id": "SOR5QmqTWpyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initiate_fine_tuning(filename: str, model_name: str) -> dict:\n",
        "    \"\"\"Initiates the fine-tuning process with OpenAI.\n",
        "\n",
        "    Args:\n",
        "        filename: The name of the training file.\n",
        "        model_name: The name of the model to be fine-tuned.\n",
        "\n",
        "    Returns:\n",
        "        The response from the fine-tuning API call.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        file_upload = openai.File.create(file=f, purpose='fine-tune')\n",
        "\n",
        "    fine_tuning_job = openai.FineTuningJob.create(\n",
        "        training_file=file_upload['id'],\n",
        "        model=model_name\n",
        "    )\n",
        "\n",
        "    return fine_tuning_job\n",
        "\n",
        "# Initiate fine-tuning\n",
        "fine_tuning_response = initiate_fine_tuning('train.jsonl', 'gpt-3.5-turbo')\n",
        "print(f'Fine Tuning job created! (job id: {fine_tuning_response[\"id\"]})')"
      ],
      "metadata": {
        "id": "Q-NiZOnRWmvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List up to 10 events from a fine-tuning job\n",
        "openai.FineTuningJob.list_events(id=fine_tuning_response['id'], limit=10)"
      ],
      "metadata": {
        "id": "RyfPpaPdYcTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the state of a fine-tune\n",
        "openai.FineTuningJob.retrieve(fine_tuning_response['id'])"
      ],
      "metadata": {
        "id": "1bwgMLbmZNo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once the fine-tune is complete, you can retrieve the name of the model\n",
        "status = openai.FineTuningJob.retrieve(fine_tuning_response['id'])\n",
        "if status['status'] == 'succeeded':\n",
        "    print('Fine-tuned model name: ' + status['fine_tuned_model'])\n",
        "    fine_tuned_model = status['fine_tuned_model']\n",
        "else:\n",
        "    print('Fine tune not complete!')"
      ],
      "metadata": {
        "id": "0QKb99e3ZS4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once the model is available, we can use the same inference generator as before!\n",
        "# All we have to do is add the new model name\n",
        "gpt3_fine_tune_infer = create_gpt3_infer(fine_tuned_model)"
      ],
      "metadata": {
        "id": "JsdZ7PP9atja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_fine_tune_0shot_results = evaluate_decoder_inference(gpt3_fine_tune_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 0, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "biQQtESUbXXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_fine_tune_1shot_results = evaluate_decoder_inference(gpt3_fine_tune_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 1, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "btU0M6lpbwjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_fine_tune_2shot_results = evaluate_decoder_inference(gpt3_fine_tune_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 2, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "E-BWXkMXbwpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_fine_tune_3shot_results = evaluate_decoder_inference(gpt3_fine_tune_infer, phrasebank_split['test'].select(range(num_test_samples)), phrasebank_split['train'], 3, \"Classify the sentiment (negative, neutral, positive) of the following statement\")"
      ],
      "metadata": {
        "id": "86yG8n9Sbwue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the ASCII table\n",
        "header = \"|  Model Name    | Average Accuracy | Average F1 Score |\"\n",
        "separator = \"-\" * len(header)\n",
        "\n",
        "# Print the ASCII table\n",
        "print(separator)\n",
        "print(header)\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 FT (n=0)| {gpt3_fine_tune_0shot_results['Average Accuracy']:<16.4f} | {gpt3_fine_tune_0shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 FT (n=1)| {gpt3_fine_tune_1shot_results['Average Accuracy']:<16.4f} | {gpt3_fine_tune_1shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 FT (n=2)| {gpt3_fine_tune_2shot_results['Average Accuracy']:<16.4f} | {gpt3_fine_tune_2shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)\n",
        "print(f\"|GPT-3.5 FT (n=3)| {gpt3_fine_tune_3shot_results['Average Accuracy']:<16.4f} | {gpt3_fine_tune_3shot_results['Average F1 Score']:<16.4f} |\")\n",
        "print(separator)"
      ],
      "metadata": {
        "id": "s_0Y-aerec7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data preparation\n",
        "n_values = [0, 1, 2, 3]  # n-values for GPT-3.5\n",
        "gpt3_base_accuracies = [\n",
        "    gpt3_0shot_results['Average Accuracy'],\n",
        "    gpt3_1shot_results['Average Accuracy'],\n",
        "    gpt3_2shot_results['Average Accuracy'],\n",
        "    gpt3_3shot_results['Average Accuracy']\n",
        "]\n",
        "gpt3_finetuned_accuracies = [\n",
        "    gpt3_fine_tune_0shot_results['Average Accuracy'],\n",
        "    gpt3_fine_tune_1shot_results['Average Accuracy'],\n",
        "    gpt3_fine_tune_2shot_results['Average Accuracy'],\n",
        "    gpt3_fine_tune_3shot_results['Average Accuracy']\n",
        "]\n",
        "roberta_accuracy = roberta_results['Average Accuracy']\n",
        "finbert_accuracy = finbert_results['Average Accuracy']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot GPT-3.5 base accuracies\n",
        "plt.plot(n_values, gpt3_base_accuracies, marker='o', linestyle='-', label='GPT-3.5 Base', color='orange')\n",
        "\n",
        "# Plot GPT-3.5 fine-tuned accuracies\n",
        "plt.plot(n_values, gpt3_finetuned_accuracies, marker='x', linestyle='-', label='GPT-3.5 Fine-Tuned', color='m')\n",
        "\n",
        "# Plot Phi-1.5 accuracies\n",
        "plt.plot(n_values, decoder_accuracies, marker='o', linestyle='-', label='Phi-1.5', color='b')\n",
        "\n",
        "# Plot horizontal lines for Roberta and Finbert\n",
        "plt.axhline(y=roberta_accuracy, color='r', linestyle='--', label='Roberta')\n",
        "plt.axhline(y=finbert_accuracy, color='g', linestyle='--', label='Finbert')\n",
        "\n",
        "# Adding labels, title, and legend\n",
        "plt.xlabel('Number of Shots (n)')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Model Accuracy Comparison with n-shot Learning')\n",
        "plt.xticks(np.arange(min(n_values), max(n_values)+1, 1))\n",
        "plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xrEAgp9yuwgI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}